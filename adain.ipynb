{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":310927,"sourceType":"datasetVersion","datasetId":130081},{"sourceId":2118595,"sourceType":"datasetVersion","datasetId":1271215},{"sourceId":13218051,"sourceType":"datasetVersion","datasetId":8378187}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# from google.colab import files\n# files.upload()\n# !mkdir -p ~/.kaggle\n# !mv kaggle.json ~/.kaggle/\n# !chmod 600 ~/.kaggle/kaggle.json\n# !kaggle datasets download -d \"ikarus777/best-artworks-of-all-time\"\n# !kaggle datasets download -dgopalbhattrai/pascal-voc-2012-dataset","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"b_AyMZ7Irh7h","outputId":"ccddd0fb-37fa-487a-fb84-b50489f00f9d","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# colab\n# !unzip best-artworks-of-all-time.zip\n# !unzip pascal-voc-2012-dataset.zip\n# kaggle\nimport os\nimport shutil\nsrc1 = '/kaggle/input/best-artworks-of-all-time'\n# src2 = '/kaggle/input/best-artworks-of-all-time/resized' #don't use\nsrc3 = '/kaggle/input/pascal-voc-2012-dataset'\ndst = '.'\n# for folder in os.listdir(src1):\n#     folder_path = os.path.join(src1, folder)\n#     if os.path.isdir(folder_path):\n#         shutil.copytree(folder_path, os.path.join(dst, folder), dirs_exist_ok=True)\n        \n# for folder in os.listdir(src2):\n#     folder_path = os.path.join(src2, folder)\n#     if os.path.isdir(folder_path):\n#         shutil.copytree(folder_path, os.path.join(dst, folder), dirs_exist_ok=True)\n        \n# for folder in os.listdir(src3):\n#     folder_path = os.path.join(src3, folder)\n#     if os.path.isdir(folder_path):\n#         shutil.copytree(folder_path, os.path.join(dst, folder), dirs_exist_ok=True)\n# !cp /kaggle/input/best-artworks-of-all-time/artists.csv ./artists.csv","metadata":{"id":"0pVpuxm7tTAr","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e85f02f7-9464-4d8a-8937-df3957e076e8","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader,Dataset\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\nfrom torchvision import transforms\nfrom torchvision import  models\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader, DistributedSampler\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os","metadata":{"id":"V0mpJ57m6wCx","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:01.592754Z","iopub.execute_input":"2025-09-30T21:24:01.593300Z","iopub.status.idle":"2025-09-30T21:24:01.597604Z","shell.execute_reply.started":"2025-09-30T21:24:01.593276Z","shell.execute_reply":"2025-09-30T21:24:01.596847Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Using {device} device')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2blE-hsw6s_n","outputId":"e74f967e-1feb-4142-abc1-a029957dfdda","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:04.546297Z","iopub.execute_input":"2025-09-30T21:24:04.546608Z","iopub.status.idle":"2025-09-30T21:24:04.550896Z","shell.execute_reply.started":"2025-09-30T21:24:04.546588Z","shell.execute_reply":"2025-09-30T21:24:04.550118Z"}},"outputs":[{"name":"stdout","text":"Using cuda device\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class AdaIN(nn.Module):\n  def __init__(self,eps):\n    super().__init__()\n    self.eps = eps\n  def forward(self,x,y):\n    if x.shape != y.shape:\n      raise ValueError(f'x and y must have the same shape x.shape {x.shape} y.shape {y.shape}')\n\n    normal_x = (x - x.mean(axis = (2,3),keepdim=True)) / (x.std(axis = (2,3),keepdim=True) + self.eps)\n    y_mean = y.mean(axis = (2,3),keepdim=True) #for each sample and each channel\n    y_std = y.std(axis = (2,3),keepdim=True) #for each sample and each channel\n    return normal_x * y_std + y_mean\nclass ContentLoss(nn.Module):\n  def __init__(self,encoder):\n    super().__init__()\n    self.encoder = encoder\n  def forward(self,y_gen,t):\n    y_gen = self.encoder(y_gen)\n    return ((y_gen-t)**2).mean()\n\nclass StyleLoss(nn.Module):\n  def __init__(self,encoder,to_layer:int):\n    super().__init__()\n    self.encoder = encoder\n    self.layers_range = range(0,to_layer)\n  def forward(self,y_gen,y_style):\n    loss = 0.0\n    for i,layer in enumerate(self.encoder):\n      if i in self.layers_range:\n        y_gen = layer(y_gen)\n        y_style = layer(y_style)\n        mean_g = y_gen.mean(dim=(2, 3))\n        mean_s = y_style.mean(dim=(2, 3))\n        std_g = y_gen.std(dim=(2, 3))\n        std_s = y_style.std(dim=(2, 3))\n        # loss += ((y_gen.mean(dim = (2,3)) - y_style.mean(dim = (2,3)))**2) + ((y_gen.std(dim = (2,3)) - y_style.std(dim = (2,3)))**2)\n        loss += ((mean_g - mean_s) ** 2 + (std_g - std_s) ** 2).sum()\n    return loss\n\n","metadata":{"id":"c1mYHW717Inw","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:06.625367Z","iopub.execute_input":"2025-09-30T21:24:06.625612Z","iopub.status.idle":"2025-09-30T21:24:06.634755Z","shell.execute_reply.started":"2025-09-30T21:24:06.625595Z","shell.execute_reply":"2025-09-30T21:24:06.634215Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# !gdown --fuzzy https://drive.google.com/file/d/11lWUMPPMinaxDBSQKgphFBGOeQHJbHOB/view?usp=sharing\n# !mv ./AdaIN.pth ./model.pth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import copy\nfrom collections import OrderedDict\n\nvgg19 = models.vgg19(weights = models.VGG19_Weights.IMAGENET1K_V1)\n\nencoder = copy.deepcopy(vgg19.features[:22])\n\ndecoder = nn.Sequential(\n    nn.Conv2d(512, 256, 3, 1, 1), nn.ReLU(inplace=False),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=False),\n    nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=False),\n    nn.Conv2d(256, 256, 3, 1, 1), nn.ReLU(inplace=False),\n    nn.Conv2d(256, 128, 3, 1, 1), nn.ReLU(inplace=False),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.Conv2d(128, 128, 3, 1, 1), nn.ReLU(inplace=False),\n    nn.Conv2d(128, 64, 3, 1, 1), nn.ReLU(inplace=False),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU(inplace=False),\n    nn.Conv2d(64, 3, 3, 1, 1),\n)\nada = AdaIN(.000001)\n\nclass AdaINModel(nn.Module):\n  \"\"\"\n  Note freeze encoder before starting\n  \"\"\"\n  def __init__(self,encoder,decoder,ada):\n    super().__init__()\n    self.encoder = encoder\n    self.decoder = decoder\n    self.ada = ada\n  def forward(self,x_content,x_style):\n    x_content = self.encoder(x_content)\n    x_style = self.encoder(x_style)\n    ada_out = self.ada(x_content,x_style)\n    x_gen = self.decoder(ada_out)\n\n    return {\n        'x_gen':x_gen,\n        'ada_out':ada_out,\n    }\n\nc_loss,s_loss = ContentLoss(encoder),StyleLoss(encoder,to_layer=len(encoder))\nmodel = AdaINModel(encoder,decoder,ada)\nfor i, layer in enumerate(model.encoder):\n    if isinstance(layer, nn.ReLU):\n        model.encoder[i] = nn.ReLU(inplace=False)\n\nmodel = torch.compile(model) # u forget this idiot\nmodel.load_state_dict(torch.load('model_weightsV5.pth'))\nc_loss,s_loss = torch.compile(c_loss),torch.compile(s_loss)\n\nc_loss.to(device)\ns_loss.to(device)\nmodel.to(device)\n","metadata":{"id":"Tod2BVuiC6SG","colab":{"base_uri":"https://localhost:8080/"},"outputId":"efd533c8-254e-4d55-cdca-3e83206509f1","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:13.046474Z","iopub.execute_input":"2025-09-30T21:24:13.046735Z","iopub.status.idle":"2025-09-30T21:24:26.436801Z","shell.execute_reply.started":"2025-09-30T21:24:13.046715Z","shell.execute_reply":"2025-09-30T21:24:26.435971Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n100%|██████████| 548M/548M [00:08<00:00, 64.6MB/s] \n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"OptimizedModule(\n  (_orig_mod): AdaINModel(\n    (encoder): Sequential(\n      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (3): ReLU()\n      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): ReLU()\n      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): ReLU()\n      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (11): ReLU()\n      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (13): ReLU()\n      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (15): ReLU()\n      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (17): ReLU()\n      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (20): ReLU()\n      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (decoder): Sequential(\n      (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (1): ReLU()\n      (2): Upsample(scale_factor=2.0, mode='nearest')\n      (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (4): ReLU()\n      (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (6): ReLU()\n      (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (8): ReLU()\n      (9): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (10): ReLU()\n      (11): Upsample(scale_factor=2.0, mode='nearest')\n      (12): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (13): ReLU()\n      (14): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (15): ReLU()\n      (16): Upsample(scale_factor=2.0, mode='nearest')\n      (17): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n      (18): ReLU()\n      (19): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    )\n    (ada): AdaIN()\n  )\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('./artists.csv')","metadata":{"id":"__w8syf1I-uS","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.hist('paintings',bins=50)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"id":"Kxdz6XeYyw7h","outputId":"eae6880e-e1b5-4ea5-e53d-12c8059dcc35","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nrandom.seed(42)\nstyle_paths = {}\nfor root,folders,files in os.walk('resized/resized'):\n  for file in files:\n    artist = file.rindex('_')\n    artist = file[:artist]\n    if file not in style_paths:\n      style_paths[artist] = style_paths.get(artist,[])+[os.path.join(root,file)]\n\nmax_images = 250\nli = []\nfor artist,images in style_paths.items():\n  if len(images) > max_images:\n    images = random.sample(images,max_images)\n  li += images\n\nstyle_images = li\nrandom.shuffle(style_images)\nlen(style_images)","metadata":{"id":"fzkaT2nXzGOf","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:26.437772Z","iopub.execute_input":"2025-09-30T21:24:26.438204Z","iopub.status.idle":"2025-09-30T21:24:26.474089Z","shell.execute_reply.started":"2025-09-30T21:24:26.438183Z","shell.execute_reply":"2025-09-30T21:24:26.473367Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"7045"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"train_image_paths = []\nfor root,folders,files in os.walk('./VOC2012_train_val/VOC2012_train_val/JPEGImages'):\n  for file in files:\n    train_image_paths.append(os.path.join(root,file))\nrandom.shuffle(train_image_paths)\nval_image_paths = train_image_paths[:1000]\ntrain_image_paths = train_image_paths[1000:]\nlen(train_image_paths)","metadata":{"id":"ABwOr68_3Qcv","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:28.709961Z","iopub.execute_input":"2025-09-30T21:24:28.710247Z","iopub.status.idle":"2025-09-30T21:24:28.759177Z","shell.execute_reply.started":"2025-09-30T21:24:28.710218Z","shell.execute_reply":"2025-09-30T21:24:28.758562Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"16125"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"from torchvision.transforms import ToTensor,Resize,Compose,RandomCrop,Lambda\nfrom PIL import Image\nimport copy\n\nclass ImagesDataset(Dataset):\n  def __init__(self,image_paths,transform=None):\n    self.image_paths = image_paths\n    self.transform = transform\n  def __len__(self):\n    return len(self.image_paths)\n\n  def __getitem__(self, index):\n    image = self.image_paths[index]\n    image = Image.open(image).convert(\"RGB\")\n    if self.transform:\n      image = Compose(self.transform)(image)\n    return image\n\ndef resizeWithAspectRatio(image):\n  # print('shape',image.shape)\n  width,height = image.shape[-1:-3:-1]\n  short_d = -1 if width < height else -2\n  long_d = -2 if short_d == -1 else -1\n  # print(f'short {short_d}, long {long_d}')\n  ratio = image.shape[long_d]/image.shape[short_d]\n  long_new_size = int(ratio*image.shape[long_d])\n  short_new_size = 256\n  if long_new_size < 256:\n    long_new_size = 256\n    short_new_size = 256 # todo keep aspect ratio\n  new_shape = list(image.shape[-2::1])\n  new_shape[long_d] = long_new_size\n  new_shape[short_d] = short_new_size\n  # print(f'newshape {new_shape}')\n\n  return Resize(new_shape)(image)\ntransform = [\n    ToTensor(),\n    Lambda(resizeWithAspectRatio),\n    RandomCrop((224,224)),\n]\n\nstyle_dataset = ImagesDataset(style_images,transform)\ntrain_dataset = ImagesDataset(train_image_paths,transform)\nsample = ImagesDataset(train_image_paths[:1000],transform)\nval_dataset = ImagesDataset(val_image_paths,transform)\n\n","metadata":{"id":"gxQZIXID4NCQ","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:31.564004Z","iopub.execute_input":"2025-09-30T21:24:31.564321Z","iopub.status.idle":"2025-09-30T21:24:31.571731Z","shell.execute_reply.started":"2025-09-30T21:24:31.564300Z","shell.execute_reply":"2025-09-30T21:24:31.570966Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"batch_size = 32\nepochs = 50\n\ntrain_data = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,drop_last=True)\ntrainSample_data = DataLoader(sample,batch_size=batch_size,shuffle=True,drop_last=True)\nval_data = DataLoader(val_dataset,batch_size=batch_size,shuffle=False,drop_last=True)\nstyle_data = DataLoader(style_dataset,batch_size=batch_size,shuffle=True,drop_last=True)","metadata":{"id":"FGXjvxo3Bmfd","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:35.612041Z","iopub.execute_input":"2025-09-30T21:24:35.612299Z","iopub.status.idle":"2025-09-30T21:24:35.617139Z","shell.execute_reply.started":"2025-09-30T21:24:35.612279Z","shell.execute_reply":"2025-09-30T21:24:35.616201Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"optmizer = torch.optim.Adam(model.parameters(),lr=1e-3,weight_decay=.01)\nscheduler = torch.optim.lr_scheduler.LinearLR(optmizer,.1,.001,total_iters= epochs*len(train_data))\n","metadata":{"id":"Yx_UZ3TUIwDs","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# pip install line-profiler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from line_profiler import LineProfiler\nfrom queue import Queue\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n\ndef train(data,y_styles,alpha,val_data = None,evaluate_every = None,accum_loss = 1):\n  import tqdm\n  model.train()\n  model.encoder.requires_grad_(False)\n  model.ada.requires_grad_(False)\n  # loop = tqdm.tqdm(range(epochs))\n  styles_iter = iter(y_styles)\n  evaluate_every = evaluate_every or len(data)\n  for epoch in range(epochs):\n    loop = tqdm.tqdm(data, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"batch\")\n    con_loss,sty_loss,eloss = '','',''\n    for i,images in enumerate(loop):\n      try:\n\n        y_style = next(styles_iter)\n        images = images.to(device)\n        y_style = y_style.to(device)\n\n        y = model(images,y_style)\n        y_gen,ada_out = y['x_gen'],y['ada_out']\n\n        content_loss = c_loss(y_gen,ada_out)/accum_loss\n        style_loss = s_loss(y_gen,y_style)/accum_loss\n        loss = content_loss + alpha*style_loss\n        loss.backward()\n\n        if i % accum_loss == 0:\n            optmizer.step()\n            if scheduler:\n                for i in range(accum_loss):\n                    scheduler.step()\n            optmizer.zero_grad()\n            \n        if val_data and ((i+1) % evaluate_every == 0):\n          con_loss,sty_loss,eloss = evaluate(val_data,y_styles,alpha)\n\n        # loop.set_description(f'Epoch {epoch}/{epochs} Batch {i/len(data)}')\n        loop.set_postfix(content_loss = content_loss.item(),style_loss = f'{style_loss.item()*alpha}/ {style_loss.item()}',loss=loss.item(),val_loss = f'{con_loss},{sty_loss},{eloss}')\n        # loop.set_postfix(content_loss = sum(closs_queue.queue)/queue_size,style_loss = sum(sloss_queue.queue)/queue_size,loss= sum(loss_queue.queue)/queue_size)\n      except StopIteration:\n        styles_iter = iter(y_styles)\n        continue\ndef evaluate(data,y_styles,alpha):\n  model.eval()\n  loss = 0.0\n  con_loss = 0.0\n  sty_loss = 0.0\n  styles_iter = iter(y_styles)\n  with torch.no_grad():\n\n    for i,images in enumerate(data):\n      try:\n        y_style = next(styles_iter)\n        images = images.to(device)\n        y_style = y_style.to(device)\n\n        y = model(images,y_style)\n        y_gen,ada_out = y['x_gen'],y['ada_out']\n        content_loss = c_loss(y_gen,ada_out)\n        style_loss = s_loss(y_gen,y_style)\n        loss += content_loss.item() + alpha*style_loss.item()\n        con_loss += content_loss.item()\n        sty_loss += style_loss.item()\n\n      except StopIteration:\n        styles_iter = iter(y_styles)\n        continue\n    loss = loss/len(data)\n    con_loss = con_loss/len(data)\n    sty_loss = sty_loss/len(data)\n    print(f'content loss {con_loss}, style loss {sty_loss}, total loss {loss}')\n    return con_loss,sty_loss,loss\n\n","metadata":{"id":"QH_iY-9zB9UT","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:24:48.816368Z","iopub.execute_input":"2025-09-30T21:24:48.816645Z","iopub.status.idle":"2025-09-30T21:24:50.023698Z","shell.execute_reply.started":"2025-09-30T21:24:48.816625Z","shell.execute_reply":"2025-09-30T21:24:50.022615Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #warmup\ndef warmup_lr(step):\n    total_steps = epochs * len(train_data)\n    return step / total_steps\nepochs = 5\noptmizer = torch.optim.Adam(model.parameters(),lr=1e-3,weight_decay=0.01)\n\n# scheduler = torch.optim.lr_scheduler.LambdaLR(optmizer,lr_lambda=warmup_lr)\nscheduler = torch.optim.lr_scheduler.LinearLR(optmizer,.001,1,total_iters= epochs*len(train_data))\n\n\ntrain(train_data,style_data,10,val_data,len(train_data)//8,2)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":532},"id":"TwmvzSczvIVY","outputId":"2b5089da-471f-4a4f-ec85-57593877445f","trusted":true,"execution":{"iopub.status.busy":"2025-09-30T21:28:22.819950Z","iopub.execute_input":"2025-09-30T21:28:22.820692Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/50:   0%|          | 0/503 [00:00<?, ?batch/s]W0930 21:28:27.902000 36 torch/_inductor/utils.py:1137] [0/0] Not enough SMs to use max_autotune_gemm mode\nEpoch 1/50:  12%|█▏        | 62/503 [04:43<3:26:08, 28.05s/batch, content_loss=7.89, loss=1.48e+5, style_loss=148051.5625/ 14805.15625, val_loss=17.28585756978681,34221.23708417339,342229.65669930365]","output_type":"stream"},{"name":"stdout","text":"content loss 17.28585756978681, style loss 34221.23708417339, total loss 342229.65669930365\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50:  25%|██▍       | 124/503 [07:39<1:42:54, 16.29s/batch, content_loss=8.84, loss=1.74e+5, style_loss=174011.484375/ 17401.1484375, val_loss=16.99770579799529,34346.89598034274,343485.9575092254]     ","output_type":"stream"},{"name":"stdout","text":"content loss 16.99770579799529, style loss 34346.89598034274, total loss 343485.9575092254\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50:  37%|███▋      | 186/503 [10:34<1:26:02, 16.29s/batch, content_loss=9.62, loss=1.86e+5, style_loss=185580.8984375/ 18558.08984375, val_loss=16.977176050986014,33164.559475806454,331662.5719341155]","output_type":"stream"},{"name":"stdout","text":"content loss 16.977176050986014, style loss 33164.559475806454, total loss 331662.5719341155\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50:  49%|████▉     | 248/503 [13:27<1:09:12, 16.28s/batch, content_loss=8.25, loss=1.52e+5, style_loss=151761.689453125/ 15176.1689453125, val_loss=17.125981576981083,34162.474609375,341641.872075327]  ","output_type":"stream"},{"name":"stdout","text":"content loss 17.125981576981083, style loss 34162.474609375, total loss 341641.872075327\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50:  62%|██████▏   | 310/503 [16:21<52:04, 16.19s/batch, content_loss=7.86, loss=1.46e+5, style_loss=145918.154296875/ 14591.8154296875, val_loss=17.064253622485744,33565.509135584674,335672.1556094693]","output_type":"stream"},{"name":"stdout","text":"content loss 17.064253622485744, style loss 33565.509135584674, total loss 335672.1556094693\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50:  74%|███████▍  | 372/503 [19:16<35:28, 16.25s/batch, content_loss=9.05, loss=1.75e+5, style_loss=175228.53515625/ 17522.853515625, val_loss=17.318275882351784,33901.33083417339,339030.6266176162]   ","output_type":"stream"},{"name":"stdout","text":"content loss 17.318275882351784, style loss 33901.33083417339, total loss 339030.6266176162\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50:  86%|████████▋ | 434/503 [22:11<18:41, 16.25s/batch, content_loss=8.1, loss=1.45e+5, style_loss=145053.02734375/ 14505.302734375, val_loss=16.72066786981398,31760.455708165322,317621.27774952305]  ","output_type":"stream"},{"name":"stdout","text":"content loss 16.72066786981398, style loss 31760.455708165322, total loss 317621.27774952305\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50:  99%|█████████▊| 496/503 [25:04<01:53, 16.18s/batch, content_loss=8.66, loss=1.6e+5, style_loss=159773.486328125/ 15977.3486328125, val_loss=17.60108200196297,34319.155367943546,343209.15476143744] ","output_type":"stream"},{"name":"stdout","text":"content loss 17.60108200196297, style loss 34319.155367943546, total loss 343209.15476143744\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/50: 100%|██████████| 503/503 [25:18<00:00,  3.02s/batch, content_loss=9.31, loss=2.15e+5, style_loss=215447.96875/ 21544.796875, val_loss=17.60108200196297,34319.155367943546,343209.15476143744]        \nEpoch 2/50:  12%|█▏        | 62/503 [02:54<1:58:24, 16.11s/batch, content_loss=8.38, loss=1.55e+5, style_loss=155468.45703125/ 15546.845703125, val_loss=17.071331147224672,33356.63942792339,333583.4656103811]","output_type":"stream"},{"name":"stdout","text":"content loss 17.071331147224672, style loss 33356.63942792339, total loss 333583.4656103811\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/50:  24%|██▎       | 119/503 [04:52<13:24,  2.10s/batch, content_loss=8.94, loss=1.79e+5, style_loss=178660.0/ 17866.0, val_loss=17.071331147224672,33356.63942792339,333583.4656103811]                  ","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"#train schedula\nepochs = 5\n# optmizer = torch.optim.Adam(model.parameters(),lr=1e-3,weight_decay=0.005)\n# scheduler = torch.optim.lr_scheduler.LinearLR(optmizer,.1,.001,total_iters= epochs*len(train_data))\ntrain(train_data,style_data,10,val_data,len(train_data)//8,2)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vqt4_qHpZQki","outputId":"473354dd-5884-4863-98b4-a6052087fd61","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" evaluate(val_data,style_data,18)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model_weightsV5.pth\")","metadata":{"id":"_ie24rsOO5Wr","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('./drive')","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLXvN83hWX66","outputId":"405064af-d5fe-41e4-e574-d7018be7a3cd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\n\nsrc = '/content/model_weights.pth'\ndst = '/content/drive/My Drive/models/AdaIN.pth'\n\nshutil.copy(src, dst)\nprint(\"File uploaded successfully!\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ex_iGDA8WvxU","outputId":"20ac0cf5-c2ef-4db2-e11a-1f43317d00c5","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n\ndef crop_to_same_size(img1: torch.Tensor, img2: torch.Tensor):\n    \"\"\"\n    Crop two images to have exactly the same size (largest crop possible).\n    Assumes input images are tensors of shape [1, C, H, W].\n    Crops centered.\n    \"\"\"\n    _, _, h1, w1 = img1.shape\n    _, _, h2, w2 = img2.shape\n\n    target_h = min(h1, h2)\n    target_w = min(w1, w2)\n\n    def center_crop(img, target_h, target_w):\n        _, _, h, w = img.shape\n        top = (h - target_h) // 2\n        left = (w - target_w) // 2\n        return img[:, :, top:top+target_h, left:left+target_w]\n\n    img1_cropped = center_crop(img1, target_h, target_w)\n    img2_cropped = center_crop(img2, target_h, target_w)\n\n    return img1_cropped, img2_cropped\n\n# Usage\n\n","metadata":{"id":"MaucfXwtZy9l","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.to(device)\ncontent = ToTensor()(Image.open('./content.jpg').convert(\"RGB\"))\nstyle = ToTensor()(Image.open('./style.jpg').convert(\"RGB\"))\norignal_image = copy.deepcopy(content).permute(1, 2, 0).numpy()\ncontent = torch.unsqueeze(content,0)\nstyle = torch.unsqueeze(style,0)\ncontent_cropped, style_cropped = crop_to_same_size(content, style)\ncontent = content_cropped.to(device)\nstyle = style_cropped.to(device)\ny_gen = model(content, style)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"o__b3nNXYL4a","outputId":"ef849e92-eabd-49ee-810a-06050ebdb5cd","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(torch.cuda.memory_allocated() / 1024**2, \"MB allocated\")\nprint(torch.cuda.memory_reserved() / 1024**2, \"MB reserved\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\n# optmizer.zero_grad(set_to_none=True)\ngc.collect()\n\ntorch.cuda.empty_cache()\n","metadata":{"id":"X3EJNtfFeYhw","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torchvision.transforms import ToTensor, Resize, CenterCrop\nfrom PIL import Image\nimport glob\nmodel.eval()\nmodel.to(device)\n# Load content image\ncontent = ToTensor()(Image.open('/kaggle/input/test-data/content.jpg').convert(\"RGB\"))\ncontent_c, content_h, content_w = content.shape\ncontent = content.unsqueeze(0)  # [1, C, H, W]\n\n# Load style images\nstyle_files = sorted(glob.glob('/kaggle/input/test-data/style*.jpg'))\nstyle_list = []\n\nfor f in style_files:\n    img = Image.open(f).convert(\"RGB\")\n    style_h, style_w = img.size[1], img.size[0]\n\n    # If style smaller than content, resize to content size (ignore aspect ratio)\n    if style_h < content_h or style_w < content_w:\n        img = img.resize((content_w, content_h))\n    # If style larger than content, center crop\n    elif style_h > content_h or style_w > content_w:\n        img = CenterCrop((content_h, content_w))(img)\n\n    style_list.append(ToTensor()(img))\n\n# Stack into batch\nstyle_batch = torch.stack(style_list, dim=0)  # [n, C, H, W]\n\n# Broadcast content to match batch size\ncontent_batch = content.repeat(style_batch.size(0), 1, 1, 1)\n\n# Move to device\nstyle_iter = iter(style_data)\ncontent_batch = next( iter(train_data) )\nstyle_batch = next(style_iter)\n\nstyle_batch = style_batch.to(device)\ncontent_batch = content_batch.to(device)\n\n# Generate stylized images\ny = None\ncontent_loss,style_loss = None, None\nwith torch.no_grad():\n    y = model(content_batch, style_batch)\n    y_gen,ada_out = y['x_gen'],y['ada_out']\n    \ncontent_loss = c_loss(y_gen,ada_out)\nstyle_loss = s_loss(y_gen,style_batch)\n    ","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"ULiHLylEeLD7","outputId":"2d68c536-478c-44f2-83fb-b3ff66493282","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"content_loss,style_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n = 16\nplt.figure(figsize=(15, 5 * n))  # Wider for side-by-side\n\nfor i in range(n):\n    # Content image\n    content_img = content_batch[i].permute(1, 2, 0).detach().cpu().numpy()\n    content_img = (content_img * 255).astype(np.uint8)\n\n    # Style image\n    style_img = style_batch[i].permute(1, 2, 0).detach().cpu().numpy()\n    style_img = (style_img * 255).astype(np.uint8)\n\n    # Generated image\n    gen_img = y_gen[i].permute(1, 2, 0).detach().cpu().numpy()\n    gen_img = (gen_img * 255).astype(np.uint8)\n\n    # Plot row: content | style | generated\n    plt.subplot(n, 3, i * 3 + 1)\n    plt.imshow(content_img)\n    plt.title(\"Content\")\n    plt.axis(\"off\")\n\n    plt.subplot(n, 3, i * 3 + 2)\n    plt.imshow(style_img)\n    plt.title(\"Style\")\n    plt.axis(\"off\")\n\n    plt.subplot(n, 3, i * 3 + 3)\n    plt.imshow(gen_img)\n    plt.title(\"Generated\")\n    plt.axis(\"off\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import math\nplt.figure(figsize=(20,20))\nplt.subplot(math.ceil(len(style_files)/4)+1,2,1)\n# plt.imshow(original_image)\n\nfor i in range(2,len(content_batch)+1):\n  image = y_gen[i-1].permute(1, 2, 0).detach().cpu().numpy()\n  image = (image*255).astype(np.uint8)\n  plt.subplot(math.ceil(len(style_files)/4)+1,2,i)\n  plt.imshow(image)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"sUisiwqUadx-","outputId":"6430e4d1-b089-4529-e9f6-6ffa587bf226","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"math.ceil(len(style_files)/4)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FuSKHUjXa-oZ","outputId":"53e29c77-c6d9-4922-c1f6-998ec6540759","trusted":true},"outputs":[],"execution_count":null}]}